import torch
import torch.nn as nn
from torch import Tensor
import torch.nn.functional as F

def one_hot(labels: Tensor, class_size: int):
  targets = torch.zeros(labels.size(0), class_size)
  for i, label in enumerate(labels):
    targets[i, label] = 1
  return targets

class CVAE(nn.Module):

  class Encoder(nn.Module): 
    """
    The encoder takes at point in image space and maps it to not a point, 
    but a distribution p(z|x) in the latent space, this is called the posterior
    distribution, which is intractable. In Kingma's paper, he just simplified 
    it to be a diagonal Gaussian q_phi (z|x), but in reality this can be anything. 
    """

    def __init__(self, idim, hdim, odim): 
      super().__init__() 
      self.odim = odim
      self.fc1 = nn.Linear(idim, hdim)
      self.fc21 = nn.Linear(hdim, odim)
      self.fc22 = nn.Linear(hdim, odim) 

    def forward(self, x: Tensor, y: Tensor): 
      y = one_hot(y, 10)
      input = torch.cat([x, y], dim=1) # concatenate across feature dim
      h1 = F.relu(self.fc1(input)) 
      z_mu = self.fc21(h1)
      z_logvar = self.fc22(h1)
      return z_mu, z_logvar

  class Decoder(nn.Module): 
    """
    Decoder takes a point generated by the distribution in the latent space, 
    and maps it back to image space. 
    """

    def __init__(self, idim, hdim, odim): 
      super().__init__()
      self.fc1 = nn.Linear(idim, hdim)
      self.fc2 = nn.Linear(hdim, odim)

    def forward(self, z, y): 
      y = one_hot(y, 10)
      input = torch.cat([z, y], dim=1) 
      h3 = F.relu(self.fc1(input))
      return torch.sigmoid(self.fc2(h3))

  def __init__(self, feature_dim, class_dim, hdim, zdim, device):
    super().__init__()
    self.idim = feature_dim + class_dim 
    self.encoder = self.Encoder(feature_dim + class_dim, hdim, zdim)
    self.decoder = self.Decoder(zdim + class_dim, hdim, feature_dim)
    self.device = device
    self.to(device)

  def encode(self, x, y): return self.encoder(x, y) 
  def decode(self, z, y): return self.decoder(z, y)

  def reparameterize(self, mu, logvar):
    """
    Sample a point from p(z|x), which is a Gaussian RV
    """
    stdev = torch.exp(0.5*logvar)
    eps = torch.randn_like(stdev)
    return mu + eps*stdev

  def forward(self, x, y):
    """
    1. Encode x to compute conditional distribution q_phi (z|x) 
    2. Sample from p(z|x) using reparameterization trick. 
    3. Decode to get p(x|z). 
    Just a composition of the encoder, reparamterization, and decoding. 
    """
    mu, logvar = self.encoder(x, y)
    z = self.reparameterize(mu, logvar)
    x_recon = self.decoder(z, y)
    return x_recon, mu, logvar

