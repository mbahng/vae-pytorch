import torch.nn.functional as F
import torch
from tqdm import tqdm
from model import VAE, CVAE
from torch.utils.data import DataLoader
from torch.optim import Optimizer

def loss_function(recon_x, x, mu, logvar):
  """
  Computes the loss = -ELBO = Negative Log-Likelihood + KL Divergence. 
  Note that this is just a balance between how close our posterior is 
  to our modeled prior (q_phi(z|x) vs p(z)), and the NLL of the samples 
  x generated by z. Eq (10) in Kingma's paper
  """
  BCE = F.binary_cross_entropy(recon_x, x, reduction='sum') # BCE = -Negative Log-likelihood
  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # KL Divergence b/w q_\phi(z|x) || p(z)
  return BCE + KLD

def train_vae(model: VAE, train_loader: DataLoader, optimizer: Optimizer):
  train_loss = 0
  model.train()
  for x, _ in tqdm(train_loader):
    x = x.to(model.device)
    x_recon, mu, logvar = model(x)

    loss = loss_function(x_recon, x , mu, logvar)
    train_loss += loss.item()

    loss.backward()
    optimizer.step()
    model.zero_grad()

  print(f'Loss: {train_loss / len(train_loader.dataset)}') # type: ignore

def test_vae(model: VAE, test_loader: DataLoader):
  model.eval()
  test_loss = 0
  with torch.no_grad():
    for x, _ in tqdm(test_loader):
      x = x.to(model.device)
      x_recon, mu, logvar = model(x)

      loss = loss_function(x_recon, x , mu, logvar)
      test_loss += loss.item() 

  print(f'Loss: {test_loss / len(test_loader.dataset)}') # type: ignore


def train_cvae(model: CVAE, train_loader: DataLoader, optimizer: Optimizer):
  train_loss = 0
  model.train()
  for x, y in tqdm(train_loader):
    x = x.to(model.device)
    x_recon, mu, logvar = model(x, y)

    loss = loss_function(x_recon, x , mu, logvar)
    train_loss += loss.item()

    loss.backward()
    optimizer.step()
    model.zero_grad()

  print(f'Loss: {train_loss / len(train_loader.dataset)}') # type: ignore

def test_cvae(model: CVAE, test_loader: DataLoader):
  model.eval()
  test_loss = 0
  with torch.no_grad():
    for x, y in tqdm(test_loader):
      x = x.to(model.device)
      x_recon, mu, logvar = model(x, y)

      loss = loss_function(x_recon, x, mu, logvar)
      test_loss += loss.item() 

  print(f'Loss: {test_loss / len(test_loader.dataset)}') # type: ignore


def train(model, dataloader: DataLoader, optimizer: Optimizer): 
  if isinstance(model, VAE):
    return train_vae(model, dataloader, optimizer) 
  elif isinstance(model, CVAE):
    return train_cvae(model, dataloader, optimizer) 
  else: 
    raise NotImplementedError()

def test(model, dataloader: DataLoader): 
  if isinstance(model, VAE):
    return test_vae(model, dataloader) 
  elif isinstance(model, CVAE):
    return test_cvae(model, dataloader) 
  else: 
    raise NotImplementedError()

