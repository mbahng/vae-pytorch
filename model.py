import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
  def __init__(self, n_in, n_hid, z_dim):
    super(VAE, self).__init__()

    self.fc1 = nn.Linear(n_in, n_hid)
    self.fc21 = nn.Linear(n_hid, z_dim)
    self.fc22 = nn.Linear(n_hid, z_dim)
    self.fc3 = nn.Linear(z_dim, n_hid)
    self.fc4 = nn.Linear(n_hid, n_in)

  def encode(self, x): 
    """
    The encoder takes at point in image space and maps it to not a point, 
    but a distribution p(z|x) in the latent space
    """
    h1 = F.relu(self.fc1(x))
    return self.fc21(h1), self.fc22(h1) # \mu, \logvar(\sigma^2)

  def reparameterize(self, mu, logvar):
    """
    Sample a point from p(z|x), which is a Gaussian RV
    """
    stdev = torch.exp(0.5*logvar)
    eps = torch.randn_like(stdev)
    return mu + eps*stdev

  def decode(self, z): 
    """
    Decoder takes a point generated by the distribution in the latent space, 
    and maps it back to image space. 
    """
    h3 = F.relu(self.fc3(z))
    return torch.sigmoid(self.fc4(h3))

  def forward(self, x):
    """
    Just a composition of the encoder, reparamterization, and decoding. 
    Note that in the computational graph
    
    """
    mu, logvar = self.encode(x)
    z = self.reparameterize(mu, logvar)
    return self.decode(z), mu, logvar

